{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uQJA17ZfEqNL",
    "outputId": "b2d444df-9db3-4aa2-8fbd-5b3b1c0e4367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.10.3)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xzELoRy1LOKo",
    "outputId": "6759f5e0-ec54-4ee6-ba4f-46dc3993f02f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.5)\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.9)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (5.10.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.10.3)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.9)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.21 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.21)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.147)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
      "Collecting openai\n",
      "  Downloading openai-1.57.2-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
      "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat) (2.21.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat) (4.23.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from nbformat) (5.7.2)\n",
      "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbformat) (5.7.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat) (0.22.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.3.6)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain) (24.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain) (3.0.0)\n",
      "Downloading langchain_openai-0.2.12-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.57.2-py3-none-any.whl (389 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.9/389.9 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken, openai, langchain-openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.54.5\n",
      "    Uninstalling openai-1.54.5:\n",
      "      Successfully uninstalled openai-1.54.5\n",
      "Successfully installed langchain-openai-0.2.12 openai-1.57.2 tiktoken-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install openai langchain langchain-openai nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6MTJYGW3LqlR",
    "outputId": "1583332c-0d7f-4006-ec31-3935bd5251b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: openai 1.57.2\n",
      "Uninstalling openai-1.57.2:\n",
      "  Successfully uninstalled openai-1.57.2\n",
      "Collecting openai\n",
      "  Using cached openai-1.57.2-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.10.3)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
      "Using cached openai-1.57.2-py3-none-any.whl (389 kB)\n",
      "Installing collected packages: openai\n",
      "Successfully installed openai-1.57.2\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y openai\n",
    "!pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z4Bt0UuCIIum",
    "outputId": "f2d9b2b6-7043-468f-ced3-f63f5b379e55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "OpenAI API key verified successfully!\n",
      "Setup completed successfully! You can now use OpenAI in your notebook.\n",
      "\n",
      "Testing connection with a simple query...\n",
      "Test successful! Response: Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "import json\n",
    "from typing import Optional\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "config_dir = '/content/drive/MyDrive/colab_config'\n",
    "key_path = f'{config_dir}/openai_key.json'\n",
    "\n",
    "def save_api_key(key: str) -> None:\n",
    "    \"\"\"Save OpenAI API key to Drive.\"\"\"\n",
    "    os.makedirs(config_dir, exist_ok=True)\n",
    "    with open(key_path, 'w') as f:\n",
    "        json.dump({'api_key': key}, f)\n",
    "\n",
    "def load_api_key() -> Optional[str]:\n",
    "    \"\"\"Load OpenAI API key from Drive.\"\"\"\n",
    "    try:\n",
    "        with open(key_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            return data['api_key']\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def test_openai_connection(api_key: str) -> bool:\n",
    "    \"\"\"Test if the OpenAI API key works.\"\"\"\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI(api_key=api_key)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Test message\"}],\n",
    "            max_tokens=5\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI API test failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def setup_openai_key() -> bool:\n",
    "    \"\"\"Setup OpenAI API key and verify it works.\"\"\"\n",
    "    api_key = load_api_key()\n",
    "\n",
    "    if not api_key:\n",
    "        print(\"Please enter your OpenAI API key:\")\n",
    "        new_key = input().strip()  # Added strip() to remove any accidental whitespace\n",
    "        api_key = new_key\n",
    "\n",
    "    if test_openai_connection(api_key):\n",
    "        save_api_key(api_key)\n",
    "        os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "        print(\"OpenAI API key verified successfully!\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "if 'OPENAI_API_KEY' in os.environ:\n",
    "    del os.environ['OPENAI_API_KEY']\n",
    "\n",
    "if not setup_openai_key():\n",
    "    print(\"Would you like to try with a new API key? (yes/no)\")\n",
    "    response = input()\n",
    "    if response.lower() == 'yes':\n",
    "        if os.path.exists(key_path):\n",
    "            os.remove(key_path)\n",
    "        if not setup_openai_key():\n",
    "            print(\"Failed to setup OpenAI API key. Please check your key and try again.\")\n",
    "else:\n",
    "    print(\"Setup completed successfully! You can now use OpenAI in your notebook.\")\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI()  # This will use the environment variable\n",
    "    print(\"\\nTesting connection with a simple query...\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Say hello\"}],\n",
    "        max_tokens=10\n",
    "    )\n",
    "    print(\"Test successful! Response:\", response.choices[0].message.content)\n",
    "except Exception as e:\n",
    "    print(f\"Error during final verification: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQQOqYl-d-4q"
   },
   "source": [
    "### Part A: Build a code understanding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_sh_FkwVd3wa"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nbformat\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "class JupyterNotebookAnalyzer:\n",
    "    def __init__(self, api_key: Optional[str] = None):\n",
    "        if api_key:\n",
    "            os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "        self.client = OpenAI()\n",
    "        self.notebook_content = None\n",
    "\n",
    "    def load_notebook(self, notebook_path: str) -> dict:\n",
    "        try:\n",
    "            with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "                self.notebook_content = nbformat.read(f, as_version=4)\n",
    "            return self.extract_notebook_content()\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error loading notebook: {str(e)}\")\n",
    "\n",
    "    def extract_notebook_content(self) -> dict:\n",
    "        content = {\n",
    "            'code_cells': [],\n",
    "            'markdown_cells': [],\n",
    "            'imports': [],\n",
    "            'functions': [],\n",
    "            'model_architecture': [],\n",
    "            'training_code': [],\n",
    "        }\n",
    "        for cell in self.notebook_content.cells:\n",
    "            if cell.cell_type == 'code':\n",
    "                code = cell.source\n",
    "                content['code_cells'].append(code)\n",
    "                if 'import' in code:\n",
    "                    content['imports'].append(code)\n",
    "                elif 'def ' in code:\n",
    "                    content['functions'].append(code)\n",
    "                elif any(term in code.lower() for term in ['model', 'sequential', 'conv2d', 'dense']):\n",
    "                    content['model_architecture'].append(code)\n",
    "                elif any(term in code.lower() for term in ['fit', 'compile', 'train', 'optimizer']):\n",
    "                    content['training_code'].append(code)\n",
    "            elif cell.cell_type == 'markdown':\n",
    "                content['markdown_cells'].append(cell.source)\n",
    "        return content\n",
    "\n",
    "    def analyze_notebook(self, question: str) -> str:\n",
    "        if not self.notebook_content:\n",
    "            raise ValueError(\"No notebook has been loaded yet\")\n",
    "        context = \"\\n=== IMPORTS ===\\n\"\n",
    "        context += \"\\n\".join(self.extract_notebook_content()['imports'])\n",
    "        context += \"\\n\\n=== MODEL ARCHITECTURE ===\\n\"\n",
    "        context += \"\\n\".join(self.extract_notebook_content()['model_architecture'])\n",
    "        context += \"\\n\\n=== TRAINING CODE ===\\n\"\n",
    "        context += \"\\n\".join(self.extract_notebook_content()['training_code'])\n",
    "        context += \"\\n\\n=== OTHER CODE ===\\n\"\n",
    "        context += \"\\n\".join([cell for cell in self.extract_notebook_content()['code_cells']\n",
    "                            if cell not in self.extract_notebook_content()['imports'] and\n",
    "                               cell not in self.extract_notebook_content()['model_architecture'] and\n",
    "                               cell not in self.extract_notebook_content()['training_code']])\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4-turbo-preview\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"\"\"You are an expert in deep learning and PyTorch/TensorFlow analysis.\n",
    "                    Focus on:\n",
    "                    - Model architecture and design choices\n",
    "                    - Training methodology and hyperparameters\n",
    "                    - Data preprocessing and augmentation\n",
    "                    - Performance optimization opportunities\n",
    "                    - Best practices in deep learning\"\"\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"\"\"\n",
    "                    Please analyze this Fashion MNIST notebook and answer the question:\n",
    "\n",
    "                    {context}\n",
    "\n",
    "                    Question: {question}\n",
    "                    \"\"\"}\n",
    "                ],\n",
    "                temperature=0.2,\n",
    "                max_tokens=2000\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Error analyzing notebook: {str(e)}\"\n",
    "\n",
    "    def get_model_summary(self) -> str:\n",
    "        return self.analyze_notebook(\"\"\"\n",
    "        Provide a detailed summary of the model architecture, including:\n",
    "        - Layer structure and parameters\n",
    "        - Design choices and their implications\n",
    "        - Potential improvements\n",
    "        \"\"\")\n",
    "\n",
    "    def get_training_analysis(self) -> str:\n",
    "        return self.analyze_notebook(\"\"\"\n",
    "        Analyze the training setup, including:\n",
    "        - Optimizer choice and hyperparameters\n",
    "        - Loss function\n",
    "        - Batch size and epochs\n",
    "        - Learning rate strategy\n",
    "        - Data preprocessing steps\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAJ1WwrreF9Z"
   },
   "source": [
    "### Upload your own custom code files to the model and ask questions based on the code file as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y79-8ATqIFIJ",
    "outputId": "9b072127-603e-4286-fe13-048ff85d51b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Architecture Summary:\n",
      "The provided notebook outlines the development and evaluation of a neural network model for the Fashion MNIST dataset using PyTorch. Below is a detailed analysis of the model architecture, design choices, and potential areas for improvement.\n",
      "\n",
      "### Model Architecture\n",
      "\n",
      "The model architecture is defined in the `MLP` class, which represents a simple Multi-Layer Perceptron (MLP) with the following layers:\n",
      "\n",
      "1. **Input Layer**: The input layer is a fully connected (linear) layer with 784 inputs (28x28 images flattened) and 256 outputs. This is an increase from an earlier version of the model which had 128 outputs. Increasing the number of outputs (neurons) in this layer allows the model to potentially learn more complex representations of the input data.\n",
      "\n",
      "2. **Hidden Layer 1**: This is another fully connected layer with 256 inputs and 64 outputs. The increase in the number of inputs (from 128 to 256) matches the increased output size of the previous layer. This layer continues the process of transforming the input data into a more abstract representation.\n",
      "\n",
      "3. **Output Layer**: The final layer is a fully connected layer with 64 inputs and 10 outputs, corresponding to the 10 classes in the Fashion MNIST dataset.\n",
      "\n",
      "Each layer is followed by a ReLU activation function, except for the output layer. If the loss function is `nn.NLLLoss`, a log-softmax function is applied to the outputs of the final layer to prepare them for the loss calculation.\n",
      "\n",
      "Additionally, dropout is applied after the first and second activation functions, with a rate that can be adjusted as a hyperparameter. Dropout is a regularization technique that helps prevent overfitting by randomly setting a fraction of input units to 0 at each update during training time.\n",
      "\n",
      "### Design Choices and Their Implications\n",
      "\n",
      "1. **Activation Function**: ReLU is used for its simplicity and effectiveness in avoiding the vanishing gradient problem, allowing for deeper networks. The option to use other activation functions like LeakyReLU, Sigmoid, or Tanh provides flexibility to experiment with different nonlinearities.\n",
      "\n",
      "2. **Dropout Rate**: Including dropout as a hyperparameter allows for tuning the model's regularization, potentially improving its generalization to unseen data.\n",
      "\n",
      "3. **Loss Function**: The use of `nn.CrossEntropyLoss` is appropriate for multi-class classification problems. The option to use `nn.NLLLoss` necessitates the application of a log-softmax layer, which is handled in the model's forward method.\n",
      "\n",
      "4. **Optimizer**: The choice of optimizer (Adam, SGD, RMSprop) and the ability to adjust learning rates and other parameters (like momentum for SGD) offers a way to experiment with different optimization strategies.\n",
      "\n",
      "### Potential Improvements\n",
      "\n",
      "1. **Layer Structure**: Experimenting with more or fewer layers, or adjusting the number of neurons in each layer, could lead to better performance. Deepening the network might capture more complex patterns, but it also risks overfitting and requires more computational resources.\n",
      "\n",
      "2. **Batch Normalization**: Introducing batch normalization layers could improve training stability and speed by normalizing the inputs of each layer. It can also act as a regularizer, potentially reducing the need for dropout.\n",
      "\n",
      "3. **Learning Rate Scheduling**: Implementing learning rate scheduling or using optimizers with adaptive learning rates (like Adam) could improve training efficiency and model performance by adjusting the learning rate based on training progress.\n",
      "\n",
      "4. **Data Augmentation**: While not directly related to the model architecture, augmenting the training data (e.g., with rotations, translations, or scaling) could improve the model's robustness and generalization by providing a more varied training dataset.\n",
      "\n",
      "5. **Advanced Architectures**: Exploring more complex architectures like Convolutional Neural Networks (CNNs), which are more suited for image data, could significantly improve performance on the Fashion MNIST dataset.\n",
      "\n",
      "6. **Hyperparameter Optimization**: Systematically searching the hyperparameter space using techniques like grid search or random search could identify better-performing models. More sophisticated methods like Bayesian optimization could be more efficient but require additional tools and resources.\n",
      "\n",
      "In summary, while the provided MLP model is a solid starting point for the Fashion MNIST classification task, there are numerous opportunities for refinement and experimentation to enhance its performance.\n",
      "\n",
      "Training Setup Analysis:\n",
      "Analyzing the provided training setup for the Fashion MNIST dataset using PyTorch involves examining several key components: optimizer choice and hyperparameters, loss function, batch size and epochs, learning rate strategy, and data preprocessing steps. Let's delve into each of these areas:\n",
      "\n",
      "### Optimizer Choice and Hyperparameters\n",
      "\n",
      "The notebook experiments with different optimizers, including Adam, SGD, and RMSprop, which are common choices for training deep neural networks. Adam is used with a learning rate of 0.001, which is a standard starting point for many tasks. SGD is used with a learning rate of 0.01 and a momentum of 0.9, providing a balance between exploration of the parameter space and exploitation of the gradients. RMSprop is also used with a learning rate of 0.001, suitable for non-stationary objectives as it adapts the learning rate for each parameter.\n",
      "\n",
      "- **Adam**: Known for its adaptive learning rate properties, making it effective in practice and requiring less tuning.\n",
      "- **SGD with Momentum**: Helps accelerate gradients in the right direction, leading to faster converging.\n",
      "- **RMSprop**: Similar to Adam, it adapts the learning rate for each weight, which helps in scenarios where the optimization landscape is irregular.\n",
      "\n",
      "### Loss Function\n",
      "\n",
      "The CrossEntropyLoss is used for the classification tasks, which is appropriate for multi-class classification problems like Fashion MNIST. It combines LogSoftmax and NLLLoss in one single class, making it convenient and efficient for training classification models.\n",
      "\n",
      "### Batch Size and Epochs\n",
      "\n",
      "A batch size of 64 is chosen, which is a reasonable size that balances the speed of computation and the stability of the gradient updates. The model is trained for 5 epochs, which might be on the lower side for achieving the best performance. However, this setting provides a quick turnaround for experimentation.\n",
      "\n",
      "### Learning Rate Strategy\n",
      "\n",
      "A fixed learning rate is used for the experiments, which is a simple and common approach. However, adopting a learning rate scheduler, such as the StepLR or ReduceLROnPlateau, could potentially improve training by adjusting the learning rate based on certain criteria (e.g., validation loss not improving).\n",
      "\n",
      "### Data Preprocessing Steps\n",
      "\n",
      "The data preprocessing involves normalizing the images with a mean of 0.5 and a standard deviation of 0.5. This normalization step is crucial as it ensures that the input features are on a similar scale, leading to a more stable and faster training process. However, the notebook does not mention any data augmentation techniques, which could be beneficial for improving the model's generalization capability by artificially increasing the diversity of the training data.\n",
      "\n",
      "### Recommendations for Improvement:\n",
      "\n",
      "1. **Data Augmentation**: Implement data augmentation strategies such as random rotations, translations, and flips to make the model more robust to variations in the input data.\n",
      "2. **Learning Rate Scheduling**: Incorporate learning rate scheduling to adjust the learning rate during training, which can lead to better model performance and faster convergence.\n",
      "3. **Increase Epochs**: Depending on the performance on the validation set, increasing the number of epochs might be beneficial until the model starts to overfit.\n",
      "4. **Experiment with Batch Size**: Experimenting with different batch sizes could potentially improve the training dynamics. Larger batch sizes provide more stable gradient estimates, while smaller batch sizes offer regularization effects.\n",
      "5. **Advanced Regularization Techniques**: Beyond dropout, other regularization techniques like weight decay (L2 regularization) or batch normalization could be explored to improve model performance.\n",
      "\n",
      "In summary, the training setup provides a solid foundation for experimenting with the Fashion MNIST dataset. By considering the recommendations for improvement, one could further enhance the model's performance and robustness.\n",
      "\n",
      "Question: What is the overall approach used for the Fashion MNIST classification?\n",
      "Response: The overall approach used for the Fashion MNIST classification in the provided notebook involves several key steps, each contributing to the development, training, and evaluation of a neural network model to classify images from the Fashion MNIST dataset. Here's a breakdown of the approach:\n",
      "\n",
      "### 1. Data Preparation\n",
      "- **Dataset Loading**: The notebook uses the QMNIST dataset, which is a variant of the MNIST dataset, for both training and testing. This might be a mistake in the notebook as the question mentions Fashion MNIST. The datasets are loaded using torchvision's datasets module, with transformations applied to convert images to tensors and normalize them.\n",
      "- **Data Augmentation**: The only preprocessing done is normalization. For Fashion MNIST, additional augmentation techniques like rotation, scaling, or horizontal flipping could potentially improve model robustness.\n",
      "- **Data Visualization**: The notebook includes code to visualize samples from the dataset, which is good practice for understanding the data.\n",
      "\n",
      "### 2. Model Architecture\n",
      "- **Initial Model**: A simple Multi-Layer Perceptron (MLP) with two hidden layers is defined. This model architecture is straightforward and suitable for a baseline model but might be limited in capturing complex patterns in image data compared to convolutional neural networks (CNNs).\n",
      "- **Modified Model**: The model's architecture is later modified to increase the size of the first hidden layer and to introduce dropout layers for regularization. This is a good strategy to combat overfitting and improve model generalization.\n",
      "- **Dynamic Model**: A more flexible model definition is introduced, allowing for dynamic selection of activation functions, dropout rates, and loss functions. This is an excellent approach for experimenting with different configurations to find the best performing model.\n",
      "\n",
      "### 3. Training Methodology\n",
      "- **Optimization and Loss**: The Adam optimizer and CrossEntropyLoss are used, which are standard choices for classification tasks. The learning rate is set to 0.001 for Adam, which is a common default value.\n",
      "- **Training Loop**: The training loop involves iterating over the dataset, computing the loss, and updating the model parameters. The training process prints the loss every 100 mini-batches, which helps in monitoring the training progress.\n",
      "\n",
      "### 4. Evaluation\n",
      "- **Accuracy Calculation**: The model's performance is evaluated on both the training and test sets by calculating the accuracy, which is the percentage of correctly classified images.\n",
      "- **Experimentation with Hyperparameters**: The notebook outlines an approach to run experiments with different combinations of optimizers, loss functions, dropout rates, and activation functions. This systematic experimentation can help in identifying the best model configuration.\n",
      "\n",
      "### Best Practices and Optimization Opportunities\n",
      "- **Convolutional Neural Networks**: For image classification tasks, CNNs are generally more effective than MLPs. Using CNNs could potentially improve model performance.\n",
      "- **Hyperparameter Tuning**: The approach to experimenting with different hyperparameters is commendable. However, using tools like grid search or random search could automate and potentially expand the hyperparameter search space.\n",
      "- **Data Augmentation**: Implementing more data augmentation techniques could improve model robustness and performance.\n",
      "- **Learning Rate Scheduling**: Implementing learning rate schedulers could improve optimization by adjusting the learning rate based on certain criteria or epochs.\n",
      "- **Regularization Techniques**: Besides dropout, other regularization techniques like L2 regularization could be explored.\n",
      "- **Batch Normalization**: Including batch normalization layers could improve training stability and performance.\n",
      "\n",
      "In summary, the notebook demonstrates a structured approach to tackling the Fashion MNIST classification task, with a focus on model experimentation and evaluation. While the initial model choice and training methodology are solid, there are opportunities for incorporating more advanced techniques and architectures to potentially improve performance.\n",
      "\n",
      "Question: How could the model architecture be improved?\n",
      "Response: The model architecture described in the notebook is a simple Multi-Layer Perceptron (MLP) for the Fashion MNIST dataset. While the MLP model is a good starting point, there are several areas where the architecture could be improved to enhance performance. Here are some suggestions:\n",
      "\n",
      "### 1. **Increase Model Complexity:**\n",
      "   - The current model is relatively simple. For more complex datasets or to capture more intricate patterns, consider increasing the model's complexity. This can be done by adding more layers or increasing the number of neurons in each layer. However, be cautious of overfitting.\n",
      "\n",
      "### 2. **Incorporate Convolutional Layers:**\n",
      "   - For image-related tasks, Convolutional Neural Networks (CNNs) are generally more effective than MLPs. Adding convolutional layers can help the model to better capture spatial hierarchies in the images. Start with a few convolutional layers and potentially add pooling layers to reduce dimensionality and increase the field of view.\n",
      "\n",
      "### 3. **Regularization Techniques:**\n",
      "   - To prevent overfitting, especially when increasing model complexity, incorporate regularization techniques such as L2 regularization, dropout (already considered in one of the dynamic model configurations), or batch normalization. Batch normalization can also help with faster convergence.\n",
      "\n",
      "### 4. **Advanced Activation Functions:**\n",
      "   - ReLU is a good starting activation function, but there are situations where other activations like LeakyReLU, ELU, or SELU could provide better performance by helping with the vanishing gradient problem or adding non-linearity.\n",
      "\n",
      "### 5. **Optimization and Learning Rate Scheduling:**\n",
      "   - While Adam is a robust optimizer, exploring others like SGD with momentum or RMSprop could yield better results in some cases. Additionally, implementing learning rate scheduling (e.g., ReduceLROnPlateau, CosineAnnealingLR) can help the model to converge faster and potentially reach a better local minimum.\n",
      "\n",
      "### 6. **Data Augmentation:**\n",
      "   - Although not directly related to model architecture, enhancing the dataset with data augmentation techniques (e.g., rotations, translations, scaling) can improve the model's generalization capabilities. This is particularly useful for image classification tasks.\n",
      "\n",
      "### 7. **Residual Connections:**\n",
      "   - For deeper models, adding residual connections can help with the vanishing gradient problem by allowing gradients to flow through the network more effectively. This is a technique borrowed from ResNet architectures and can be particularly beneficial in deeper networks.\n",
      "\n",
      "### 8. **Attention Mechanisms:**\n",
      "   - Incorporating attention mechanisms can allow the model to focus on more relevant parts of the image for making predictions. This is more advanced and might be more applicable to complex or larger images than Fashion MNIST.\n",
      "\n",
      "### Best Practices in Deep Learning:\n",
      "- **Experiment Systematically:** When modifying the architecture, it's crucial to change one aspect at a time and monitor the impact on performance. This systematic approach helps in understanding what works best for your specific problem.\n",
      "- **Validation Set:** Use a validation set to tune hyperparameters and avoid overfitting on the test set. This helps in developing a model that generalizes well to unseen data.\n",
      "- **Reproducibility:** Set random seeds where possible to ensure experiments are reproducible.\n",
      "- **Regular Evaluation:** Frequently evaluate the model during training to monitor progress and adjust strategies as needed.\n",
      "\n",
      "Improving a model's architecture requires experimentation and iteration. The suggestions above provide a starting point, but the best configuration often depends on the specific characteristics of the dataset and the task at hand.\n",
      "\n",
      "Question: Are there any potential overfitting issues?\n",
      "Response: Analyzing the provided notebook, which involves training a neural network on the Fashion MNIST dataset using PyTorch, several aspects can be evaluated for potential overfitting issues:\n",
      "\n",
      "### Model Architecture\n",
      "\n",
      "The model architecture is a simple Multi-Layer Perceptron (MLP) with fully connected layers. Overfitting in such models can occur if the model is too complex (i.e., has too many parameters) relative to the amount of training data. However, the MLP used here seems reasonably sized for the Fashion MNIST dataset, which is relatively simple. The use of dropout layers can help mitigate overfitting by randomly setting input units to 0 at each step during training time, which helps to prevent complex co-adaptations on training data.\n",
      "\n",
      "### Training Methodology and Hyperparameters\n",
      "\n",
      "- **Epochs**: The model is trained for a relatively small number of epochs (5), which is unlikely to lead to significant overfitting. However, without a validation set to monitor for overfitting during training, it's hard to make definitive conclusions.\n",
      "- **Optimizer and Learning Rate**: The use of the Adam optimizer with a learning rate of 0.001 is standard and generally effective for a wide range of problems. Adjusting the learning rate or using learning rate scheduling could provide better control over convergence and potentially reduce overfitting.\n",
      "- **Loss Function**: CrossEntropyLoss is appropriate for classification tasks and does not directly contribute to overfitting.\n",
      "\n",
      "### Data Preprocessing and Augmentation\n",
      "\n",
      "Data augmentation is a powerful technique to increase the diversity of your training set by applying random transformations such as rotation, scaling, and flipping. However, in the provided notebook, only basic transformations (normalization) are applied. Incorporating more augmentation could help reduce overfitting by making the model more robust to variations in the input data.\n",
      "\n",
      "### Performance Optimization Opportunities\n",
      "\n",
      "- **Regularization**: Besides dropout, other regularization techniques like L2 regularization (weight decay) could be applied to the optimizer to help control overfitting.\n",
      "- **Data Augmentation**: As mentioned, increasing data augmentation could help improve model generalization.\n",
      "- **Early Stopping**: Implementing early stopping by monitoring performance on a validation set could prevent overfitting by halting training when the validation performance begins to degrade.\n",
      "\n",
      "### Best Practices in Deep Learning\n",
      "\n",
      "- **Validation Set**: Splitting the dataset into training, validation, and test sets is crucial for monitoring overfitting during training. The validation set can be used to fine-tune hyperparameters and decide when to stop training to avoid overfitting.\n",
      "- **Batch Normalization**: Incorporating batch normalization layers could improve training stability and performance, potentially allowing the model to learn better with less risk of overfitting.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "While the notebook does not show clear signs of overfitting from the provided information (e.g., training for a limited number of epochs and using dropout), the lack of a validation set for early stopping or hyperparameter tuning, limited data augmentation, and absence of other regularization techniques could potentially lead to overfitting issues, especially if the model were trained for more epochs or on a more complex dataset. Implementing the suggested best practices would help mitigate these risks and improve model robustness and generalization.\n",
      "\n",
      "Question: What data augmentation techniques are used?\n",
      "Response: The provided notebook does not explicitly implement any data augmentation techniques for the Fashion MNIST dataset. Data augmentation refers to the process of artificially increasing the diversity of the training dataset by applying random transformations (such as rotation, scaling, cropping, flipping, etc.) that preserve the label of the data. This is a common practice to improve model generalization and robustness, especially in the context of image data.\n",
      "\n",
      "In the \"TRAINING CODE\" section, the only preprocessing steps applied to the QMNIST dataset (which seems to be a typo or confusion since the question mentions Fashion MNIST, but the code references QMNIST) are:\n",
      "\n",
      "```python\n",
      "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
      "```\n",
      "\n",
      "This code snippet indicates two operations:\n",
      "1. `transforms.ToTensor()`: Converts the images to PyTorch tensors.\n",
      "2. `transforms.Normalize((0.5,), (0.5,))`: Normalizes the images by setting their mean to 0.5 and their standard deviation to 0.5. This normalization is applied channel-wise, but given the grayscale nature of QMNIST/Fashion MNIST images, it's effectively a single-channel transformation.\n",
      "\n",
      "Neither of these steps constitutes data augmentation. They are standard data preprocessing steps. Data augmentation would involve using transforms such as `transforms.RandomHorizontalFlip()`, `transforms.RandomRotation()`, or `transforms.RandomResizedCrop()`, none of which are present in the provided code.\n",
      "\n",
      "To incorporate data augmentation into the notebook for the Fashion MNIST dataset, one could modify the `transform` variable in the \"TRAINING CODE\" section as follows:\n",
      "\n",
      "```python\n",
      "transform = transforms.Compose([\n",
      "    transforms.RandomHorizontalFlip(),  # Randomly flips the image horizontally.\n",
      "    transforms.RandomRotation(10),  # Randomly rotates the image by up to 10 degrees.\n",
      "    transforms.ToTensor(),\n",
      "    transforms.Normalize((0.5,), (0.5,))\n",
      "])\n",
      "```\n",
      "\n",
      "This modification would introduce data augmentation to the training process, potentially improving the model's ability to generalize from the training data to unseen data by exposing it to a wider variety of input scenarios.\n",
      "\n",
      "Question: How does the learning rate schedule work?\n",
      "Response: The provided notebook does not explicitly define a learning rate schedule for the optimization process. Instead, it uses a fixed learning rate for the Adam optimizer (`lr=0.001`) throughout the training process. A learning rate schedule is a strategy to adjust the learning rate during training, typically to improve convergence and possibly achieve better performance by starting with a higher learning rate and decreasing it according to a certain policy (e.g., exponentially, step decay, or by monitoring a plateau in validation loss).\n",
      "\n",
      "### Best Practices for Implementing Learning Rate Schedules in Deep Learning:\n",
      "\n",
      "1. **Use Predefined Schedules**: Frameworks like PyTorch and TensorFlow offer predefined learning rate schedules such as `StepLR`, `ExponentialLR`, and `ReduceLROnPlateau`. These can be easily integrated into the training loop to adjust the learning rate based on epoch number or performance metrics.\n",
      "\n",
      "2. **Warm-up Phases**: Starting training with a very small learning rate and gradually increasing it to a target learning rate can help in stabilizing the training process in the initial epochs. This is particularly useful for very deep networks or when training from scratch.\n",
      "\n",
      "3. **Adaptive Learning Rate Schedules**: Methods like `ReduceLROnPlateau` monitor a performance metric (e.g., validation loss) and reduce the learning rate when the metric stops improving. This approach can help in fine-tuning the model towards the end of the training process.\n",
      "\n",
      "4. **Cyclical Learning Rates**: Proposed by Leslie N. Smith, cyclical learning rates involve cyclically varying the learning rate between two bounds. This approach can lead to faster convergence and can reduce the need for extensive hyperparameter tuning.\n",
      "\n",
      "5. **Custom Schedules**: For specific use cases, custom learning rate schedules can be implemented. This involves manually adjusting the learning rate within the training loop based on the current epoch or performance metrics.\n",
      "\n",
      "### Implementing a Learning Rate Schedule in PyTorch:\n",
      "\n",
      "To integrate a learning rate schedule into the provided notebook, you can follow these steps:\n",
      "\n",
      "1. **Choose a Schedule**: Decide on a learning rate schedule based on the training behavior and the problem at hand. For example, `torch.optim.lr_scheduler.StepLR` reduces the learning rate by a factor every few epochs.\n",
      "\n",
      "2. **Initialize the Scheduler**: After defining the optimizer, initialize the scheduler. For example:\n",
      "   ```python\n",
      "   scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
      "   ```\n",
      "\n",
      "3. **Update the Learning Rate**: In the training loop, after `optimizer.step()`, update the learning rate by calling `scheduler.step()`.\n",
      "\n",
      "4. **Monitor and Adjust**: Monitor the training performance. If the model is not converging as expected, consider adjusting the schedule parameters or trying a different schedule.\n",
      "\n",
      "### Conclusion:\n",
      "\n",
      "While the notebook does not utilize a learning rate schedule, incorporating one could potentially improve model performance by allowing more nuanced control over the learning rate throughout training. This can lead to faster convergence and better overall results, especially for complex models and datasets.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    analyzer = JupyterNotebookAnalyzer()\n",
    "    notebook_path = '/content/drive/My Drive/Deep Learning/Assignment/HW12/FMNIST.ipynb'\n",
    "    try:\n",
    "        # Load the notebook\n",
    "        analyzer.load_notebook(notebook_path)\n",
    "        # Get model architecture summary\n",
    "        print(\"\\nModel Architecture Summary:\")\n",
    "        print(analyzer.get_model_summary())\n",
    "        # Get training analysis\n",
    "        print(\"\\nTraining Setup Analysis:\")\n",
    "        print(analyzer.get_training_analysis())\n",
    "        # Ask specific questions\n",
    "        questions = [\n",
    "            \"What is the overall approach used for the Fashion MNIST classification?\",\n",
    "            \"How could the model architecture be improved?\",\n",
    "            \"Are there any potential overfitting issues?\",\n",
    "            \"What data augmentation techniques are used?\",\n",
    "            \"How does the learning rate schedule work?\"\n",
    "        ]\n",
    "        for question in questions:\n",
    "            print(f\"\\nQuestion: {question}\")\n",
    "            response = analyzer.analyze_notebook(question)\n",
    "            print(f\"Response: {response}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uzpJv0JlM6sg",
    "outputId": "d715dfc6-d431-4b7b-cae6-5fd7ea357e78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.9)\n",
      "Collecting langchain_community\n",
      "  Downloading langchain_community-0.3.11-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.10/dist-packages (0.2.12)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.9)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.21 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.21)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.147)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.10.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain_community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.11-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-core<0.4.0,>=0.3.21 (from langchain)\n",
      "  Downloading langchain_core-0.3.24-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
      "  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.55.3 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (1.57.2)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.8.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.28.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain_openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain_openai) (0.8.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain_openai) (4.66.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.9.11)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.55.3->langchain_openai) (1.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain) (3.0.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading langchain_community-0.3.11-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain-0.3.11-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading langchain_core-0.3.24-py3-none-any.whl (410 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
      "Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, faiss-cpu, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, langchain, langchain_community\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.21\n",
      "    Uninstalling langchain-core-0.3.21:\n",
      "      Successfully uninstalled langchain-core-0.3.21\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.3.9\n",
      "    Uninstalling langchain-0.3.9:\n",
      "      Successfully uninstalled langchain-0.3.9\n",
      "Successfully installed dataclasses-json-0.6.7 faiss-cpu-1.9.0.post1 httpx-sse-0.4.0 langchain-0.3.11 langchain-core-0.3.24 langchain_community-0.3.11 marshmallow-3.23.1 mypy-extensions-1.0.0 pydantic-settings-2.6.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain_community langchain_openai faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UYc2RPcens2"
   },
   "source": [
    "### Part B: Write a chatbot prompt to iteratively create a sequence of chats on one particular custom data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ueCnxMH1ekYO"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from google.colab import drive\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import os\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "class EfficientSequentialChatbot:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.llm = ChatOpenAI(\n",
    "                temperature=0.5,\n",
    "                model_name=\"gpt-3.5-turbo\"\n",
    "            )\n",
    "            self.memory = ConversationBufferMemory(\n",
    "                memory_key=\"chat_history\",\n",
    "                return_messages=True\n",
    "            )\n",
    "            self.conversation_history = []\n",
    "        except Exception as e:\n",
    "            print(f\"Initialization error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def load_drive_document(self, file_path: str):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                print(f\"Document loaded successfully\")\n",
    "            text_splitter = CharacterTextSplitter(\n",
    "                chunk_size=300,\n",
    "                chunk_overlap=50\n",
    "            )\n",
    "            chunks = text_splitter.split_text(text)\n",
    "            embeddings = OpenAIEmbeddings()\n",
    "            self.vectorstore = FAISS.from_texts(chunks, embeddings)\n",
    "            self.qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "                self.llm,\n",
    "                self.vectorstore.as_retriever(search_kwargs={\"k\": 1}),  # Limit to 1 chunk for efficiency\n",
    "                memory=self.memory\n",
    "            )\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading document: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "    def get_response(self, question: str) -> str:\n",
    "        try:\n",
    "            response = self.qa_chain({\"question\": question})\n",
    "            return response[\"answer\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting response: {str(e)}\")\n",
    "            return \"I apologize, but I encountered an error. Please try asking your question again.\"\n",
    "\n",
    "    def chat_sequence(self):\n",
    "        print(\"\\nStarting interactive chat sequence. Type 'exit' to end, 'summary' for current summary.\")\n",
    "        question_count = 0\n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"\\nYour question (or 'exit'/'summary'): \").strip()\n",
    "                if user_input.lower() == 'exit':\n",
    "                    break\n",
    "                elif user_input.lower() == 'summary':\n",
    "                    print(\"\\nCurrent Conversation Summary:\")\n",
    "                    print(self.get_summary())\n",
    "                    continue\n",
    "                question_count += 1\n",
    "                print(f\"\\nQuestion {question_count}: {user_input}\")\n",
    "                response = self.get_response(user_input)\n",
    "                print(f\"Answer: {response}\")\n",
    "                self.conversation_history.append({\n",
    "                    \"question_number\": question_count,\n",
    "                    \"question\": user_input,\n",
    "                    \"answer\": response\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error during chat: {str(e)}\")\n",
    "                print(\"You can continue with your next question.\")\n",
    "\n",
    "    def get_summary(self) -> str:\n",
    "        if not self.conversation_history:\n",
    "            return \"No conversation to summarize yet.\"\n",
    "        conversation = \"\\n\".join([\n",
    "            f\"Q: {entry['question']}\\nA: {entry['answer']}\"\n",
    "            for entry in self.conversation_history\n",
    "        ])\n",
    "        summary_prompt = \"\"\"Analyze this conversation and provide a structured summary that includes:\n",
    "        1. Main Topics: What were the key subjects discussed?\n",
    "        2. Progression: How did the understanding develop through the conversation?\n",
    "        3. Key Insights: What were the important findings or revelations?\n",
    "        4. Conclusions: What are the main takeaways?\n",
    "\n",
    "        Please provide this analysis based on the following conversation:\n",
    "\n",
    "        {conversation}\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.llm.invoke(summary_prompt)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating analytical summary: {str(e)}\")\n",
    "            basic_summary = \"\\nBasic Conversation Summary:\\n\\n\"\n",
    "            basic_summary += f\"Total questions asked: {len(self.conversation_history)}\\n\"\n",
    "            basic_summary += \"\\nKey Questions Discussed:\\n\"\n",
    "            for entry in self.conversation_history:\n",
    "                basic_summary += f\"- {entry['question']}\\n\"\n",
    "            return basic_summary\n",
    "\n",
    "    def save_conversation(self, filename: str = \"chat_history.json\"):\n",
    "        try:\n",
    "            output_path = os.path.join('/content/drive/My Drive/Deep Learning/Assignment/HW12', filename)\n",
    "            data = {\n",
    "                \"conversation_history\": self.conversation_history,\n",
    "                \"summary\": self.get_summary()\n",
    "            }\n",
    "            with open(output_path, 'w') as f:\n",
    "                json.dump(data, f, indent=2)\n",
    "            print(f\"\\nConversation saved to: {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving conversation: {str(e)}\")\n",
    "            print(\"Try saving to a different location or checking file permissions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6CZmX03TNUR6",
    "outputId": "f041e355-b524-4cdf-cb6f-2d16d6743dcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Document loaded successfully\n",
      "\n",
      "Chat with the document. Type 'exit' to end, 'summary' to see conversation summary.\n",
      "\n",
      "Starting interactive chat sequence. Type 'exit' to end, 'summary' for current summary.\n",
      "\n",
      "Your question (or 'exit'/'summary'): What is the main topic of the document?\n",
      "\n",
      "Question 1: What is the main topic of the document?\n",
      "Answer: The main topic of the document is climate change, specifically focusing on how it is a global challenge primarily caused by human activities increasing greenhouse gas concentrations in the atmosphere.\n",
      "\n",
      "Your question (or 'exit'/'summary'): How does this topic affect global systems?\n",
      "\n",
      "Question 2: How does this topic affect global systems?\n",
      "Answer: Climate change, caused by human activities increasing greenhouse gas concentrations in the atmosphere, affects global systems in various ways. Some of the impacts include:\n",
      "\n",
      "1. **Rising temperatures:** Increased greenhouse gases trap heat in the atmosphere, leading to rising global temperatures. This can result in more frequent and intense heatwaves, affecting ecosystems, agriculture, and human health.\n",
      "\n",
      "2. **Melting ice and rising sea levels:** Higher temperatures cause ice caps and glaciers to melt, leading to rising sea levels. This can result in coastal flooding, loss of land, and displacement of communities.\n",
      "\n",
      "3. **Changes in precipitation patterns:** Climate change can alter precipitation patterns, leading to more intense rainfall in some regions and droughts in others. This can impact water availability, agriculture, and ecosystems.\n",
      "\n",
      "4. **Ocean acidification:** Increased levels of carbon dioxide in the atmosphere are absorbed by the oceans, leading to ocean acidification. This can harm marine life, particularly organisms that rely on calcium carbonate to build their shells and skeletons.\n",
      "\n",
      "5. **Extreme weather events:** Climate change is associated with an increase in extreme weather events such as hurricanes, droughts, wildfires, and heavy rainfall. These events can cause widespread damage to infrastructure, agriculture, and ecosystems.\n",
      "\n",
      "Overall, climate change caused by human activities has far-reaching effects on global systems, impacting ecosystems, economies, and human societies.\n",
      "\n",
      "Your question (or 'exit'/'summary'): What solutions are proposed?\n",
      "\n",
      "Question 3: What solutions are proposed?\n",
      "Answer: Some proposed solutions for the impacts of climate change caused by human activities include:\n",
      "\n",
      "1. Renewable energy adoption: Transitioning from fossil fuels to renewable energy sources such as solar, wind, and hydroelectric power can help reduce greenhouse gas emissions.\n",
      "\n",
      "2. Energy efficiency improvements: Implementing energy-efficient technologies and practices can lower energy consumption and reduce carbon emissions.\n",
      "\n",
      "3. Forest conservation and reforestation: Protecting existing forests and planting new trees can help sequester carbon dioxide from the atmosphere.\n",
      "\n",
      "4. Sustainable agriculture practices: Adopting sustainable farming methods can reduce emissions from agriculture and help build resilience to climate change.\n",
      "\n",
      "5. Carbon pricing and emissions trading: Implementing policies such as carbon pricing or emissions trading can create financial incentives for companies to reduce their greenhouse gas emissions.\n",
      "\n",
      "Your question (or 'exit'/'summary'): How effective are these solutions?\n",
      "\n",
      "Question 4: How effective are these solutions?\n",
      "Answer: The effectiveness of the proposed solutions for the impacts of climate change caused by human activities can vary depending on various factors such as implementation, scale, and integration with other solutions. Generally, a combination of these solutions is considered more effective in addressing climate change than relying on just one. It is important to continuously evaluate and adjust these solutions based on scientific evidence and real-world outcomes to maximize their impact.\n",
      "\n",
      "Your question (or 'exit'/'summary'): What are the future implications?\n",
      "\n",
      "Question 5: What are the future implications?\n",
      "Answer: Future implications of climate change include rising global temperatures leading to more extreme weather events, sea level rise causing coastal flooding, loss of biodiversity, and challenges to food and water security. Proposed solutions for addressing climate change include transitioning to renewable energy sources, implementing sustainable land use practices, increasing energy efficiency, promoting green transportation, and international cooperation to reduce greenhouse gas emissions. These solutions aim to mitigate the impacts of climate change and work towards a more sustainable future.\n",
      "\n",
      "Your question (or 'exit'/'summary'): exit\n",
      "\n",
      "Final Conversation Summary:\n",
      "A: Hey, how was your weekend?\n",
      "B: It was good, I went hiking with some friends. How about you?\n",
      "A: I just stayed home and caught up on some reading. Have you been to that new coffee shop downtown?\n",
      "B: Yeah, I went there last week. The coffee was great but the service was a bit slow.\n",
      "A: That's good to know. I'll keep that in mind if I decide to check it out. Did you hear about the new restaurant opening up next month?\n",
      "B: Yeah, I saw the sign when I was downtown. It looks like they're going to have a really interesting menu.\n",
      "A: I'm looking forward to trying it out. We should go together sometime.\n",
      "\n",
      "1. Main Topics:\n",
      "   - Weekend activities\n",
      "   - Coffee shop experience\n",
      "   - New restaurant opening\n",
      "\n",
      "2. Progression:\n",
      "   The conversation starts with a casual inquiry about the weekend, leading to a discussion about hiking and reading. The topic then shifts to a new coffee shop downtown, with one person sharing their experience and the other taking note of the feedback. The conversation transitions to a new restaurant opening, with both expressing interest in trying it out together.\n",
      "\n",
      "3. Key Insights:\n",
      "   - The coffee shop had good coffee but slow service, which could impact someone's decision to visit.\n",
      "   - The new restaurant opening has caught the interest of both individuals, suggesting a shared enthusiasm for trying new dining experiences.\n",
      "\n",
      "4. Conclusions:\n",
      "   - Feedback about local businesses, such as the coffee shop, can influence decisions about where to go.\n",
      "   - Shared interests in trying new restaurants can lead to potential plans for future outings.\n",
      "\n",
      "Conversation saved to: /content/drive/My Drive/Deep Learning/Assignment/HW12/chat_history.json\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    try:\n",
    "        drive.mount('/content/drive')\n",
    "        # Initialize chatbot\n",
    "        chatbot = EfficientSequentialChatbot()\n",
    "        # Load document\n",
    "        file_path = '/content/drive/My Drive/Deep Learning/Assignment/HW12/climate_change.txt'\n",
    "        if not chatbot.load_drive_document(file_path):\n",
    "            return\n",
    "        # Start interactive chat\n",
    "        print(\"\\nChat with the document. Type 'exit' to end, 'summary' to see conversation summary.\")\n",
    "        chatbot.chat_sequence()\n",
    "        # Final summary and save\n",
    "        print(\"\\nFinal Conversation Summary:\")\n",
    "        print(chatbot.get_summary())\n",
    "        chatbot.save_conversation()\n",
    "    except Exception as e:\n",
    "        print(f\"Main execution error: {str(e)}\")\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# What is the main topic of the document?\n",
    "# How does this topic affect global systems?\n",
    "# What solutions are proposed?\n",
    "# How effective are these solutions?\n",
    "# What are the future implications?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
